# ü§ó Setup HuggingFace Inference API

H∆∞·ªõng d·∫´n setup HuggingFace ƒë·ªÉ s·ª≠ d·ª•ng Large Language Model cho Q&A system.

---

## üéØ T·∫°i Sao D√πng HuggingFace?

-   ‚úÖ **Free tier**: ~30,000 tokens/th√°ng mi·ªÖn ph√≠
-   ‚úÖ **Serverless**: Kh√¥ng c·∫ßn host model
-   ‚úÖ **Nhi·ªÅu models**: Vietnamese LLM (VietGPT, PhoGPT, etc.)
-   ‚úÖ **D·ªÖ migrate**: Sau n√†y chuy·ªÉn sang AWS ch·ªâ c·∫ßn ƒë·ªïi endpoint

---

## üìã B∆∞·ªõc 1: T·∫°o HuggingFace Account

1. Truy c·∫≠p: https://huggingface.co
2. Click **"Sign Up"** (g√≥c ph·∫£i tr√™n)
3. ƒêƒÉng k√Ω v·ªõi:

    - Email + Password
    - GitHub (khuy·∫øn ngh·ªã)
    - Google

4. Verify email

---

## üîë B∆∞·ªõc 2: T·∫°o Access Token

1. Login v√†o HuggingFace
2. Click v√†o **Avatar** (g√≥c ph·∫£i tr√™n) ‚Üí **Settings**
3. Trong menu b√™n tr√°i, ch·ªçn **"Access Tokens"**
4. Click **"New token"**

ƒêi·ªÅn th√¥ng tin:

```
Name: vn-law-mini
Role: read (ƒë·ªß cho inference)
```

5. Click **"Generate token"**
6. **COPY TOKEN NGAY** (ch·ªâ hi·ªán 1 l·∫ßn!)

```
hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

‚ö†Ô∏è **L∆ØU √ù**: Token n√†y c√≥ quy·ªÅn truy c·∫≠p t√†i kho·∫£n c·ªßa b·∫°n, gi·ªØ b√≠ m·∫≠t!

---

## üß™ B∆∞·ªõc 3: Test Inference API

### Option A: Test qua Web UI

1. V√†o model page: https://huggingface.co/vinai/phobert-base-v2
2. Trong tab **"Hosted inference API"** b√™n ph·∫£i
3. Nh·∫≠p text ti·∫øng Vi·ªát ƒë·ªÉ test
4. N·∫øu th·∫•y k·∫øt qu·∫£ ‚Üí API ho·∫°t ƒë·ªông ‚úÖ

### Option B: Test qua Python

```bash
pip install requests
```

```python
# test_hf_api.py
import requests
import os

API_URL = "https://api-inference.huggingface.co/models/VietAI/viet-gpt-2"
headers = {"Authorization": f"Bearer {os.getenv('HF_API_TOKEN')}"}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload)
    return response.json()

# Test
output = query({
    "inputs": "Xin ch√†o, t√¥i l√†"
})

print("‚úÖ HuggingFace API connection successful!")
print(f"Output: {output}")
```

```bash
export HF_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
python test_hf_api.py
```

---

## ü§ñ B∆∞·ªõc 4: Ch·ªçn Model Ph√π H·ª£p

### C√°c models ti·∫øng Vi·ªát khuy·∫øn ngh·ªã:

| Model                       | Size | Use Case                | Free Tier?      |
| --------------------------- | ---- | ----------------------- | --------------- |
| **arcee-ai/Arcee-VyLinh**   | N/A  | Vietnamese legal domain | ‚úÖ T·ªët nh·∫•t     |
| **VietAI/viet-gpt-2**       | 110M | Fast generation         | ‚úÖ R·∫•t t·ªët      |
| **vinai/phobert-base-v2**   | 135M | Understanding           | ‚úÖ R·∫•t t·ªët      |
| **Viet-Mistral/Vistral-7B** | 7B   | Best quality            | ‚ö†Ô∏è Slow/Limited |

**Khuy·∫øn ngh·ªã cho demo**: `arcee-ai/Arcee-VyLinh` (ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·∫∑c bi·ªát cho ph√°p lu·∫≠t Vi·ªát Nam)

---

## üìù B∆∞·ªõc 5: Setup cho RAG System

### Code m·∫´u cho Q&A:

```python
# backend/rag-service/model_client.py
import requests
import os

class ModelClient:
    def __init__(self, provider='huggingface'):
        self.provider = provider
        if provider == 'huggingface':
            self.api_url = os.getenv(
                "HF_INFERENCE_API",
                "https://api-inference.huggingface.co/models/arcee-ai/Arcee-VyLinh"
            )
            self.headers = {
                "Authorization": f"Bearer {os.getenv('HF_API_TOKEN')}"
            }

    def generate(self, prompt, max_length=512, temperature=0.7):
        """
        Generate answer from prompt
        """
        if self.provider == 'huggingface':
            return self._call_hf(prompt, max_length, temperature)
        elif self.provider == 'aws':
            # Implement later when migrate to AWS
            return self._call_aws(prompt, max_length, temperature)
        else:
            raise ValueError(f"Unknown provider: {self.provider}")

    def _call_hf(self, prompt, max_length, temperature):
        payload = {
            "inputs": prompt,
            "parameters": {
                "max_length": max_length,
                "temperature": temperature,
                "do_sample": True,
                "top_p": 0.9,
                "return_full_text": False
            }
        }

        response = requests.post(
            self.api_url,
            headers=self.headers,
            json=payload,
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            if isinstance(result, list) and len(result) > 0:
                return result[0].get('generated_text', '').strip()
            return ""
        else:
            raise Exception(f"HuggingFace API error: {response.status_code} - {response.text}")

    def _call_aws(self, prompt, max_length, temperature):
        # TODO: Implement AWS endpoint when migrating
        raise NotImplementedError("AWS integration coming soon")
```

### Usage trong RAG:

```python
from model_client import ModelClient

# Initialize
llm = ModelClient(provider='huggingface')

# RAG prompt
context = "ƒêi·ªÅu 1. B·ªô lu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ quan h·ªá d√¢n s·ª±..."
question = "Ph·∫°m vi ƒëi·ªÅu ch·ªânh c·ªßa B·ªô lu·∫≠t D√¢n s·ª± l√† g√¨?"

prompt = f"""D·ª±a v√†o vƒÉn b·∫£n sau ƒë√¢y:
{context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:"""

# Generate answer
answer = llm.generate(prompt, max_length=300)
print(f"Answer: {answer}")
```

---

## üí∞ B∆∞·ªõc 6: Hi·ªÉu Rate Limits & Pricing

### Free Tier:

-   **Requests**: Kh√¥ng gi·ªõi h·∫°n s·ªë l∆∞·ª£ng
-   **Compute**: ~30,000 tokens/th√°ng
-   **Rate limit**: ~10 requests/ph√∫t cho cold models
-   **Model loading**: 20s ƒë·∫ßu ti√™n (cold start)

### Khi V∆∞·ª£t Quota:

-   L·ªói 429: "Rate limit exceeded"
-   Gi·∫£i ph√°p:
    1. D√πng cache (Redis) ƒë·ªÉ gi·∫£m calls
    2. Upgrade l√™n Pro ($9/th√°ng): 1M tokens
    3. Migrate sang AWS SageMaker

### Tips Ti·∫øt Ki·ªám:

-   ‚úÖ Cache c√¢u tr·∫£ l·ªùi v·ªõi Redis
-   ‚úÖ Gi·ªõi h·∫°n `max_length` output
-   ‚úÖ D√πng model nh·ªè h∆°n n·∫øu ƒë·ªß quality
-   ‚úÖ Implement retry logic v·ªõi exponential backoff

---

## üîê B∆∞·ªõc 7: Environment Variables

Th√™m v√†o `.env`:

```bash
# HuggingFace Configuration
HF_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
HF_INFERENCE_API=https://api-inference.huggingface.co/models/arcee-ai/Arcee-VyLinh

# Fallback model (optional)
HF_FALLBACK_MODEL=VietAI/viet-gpt-2
```

---

## üîß B∆∞·ªõc 8: Handle Errors & Retries

```python
import time
from functools import wraps

def retry_on_error(max_retries=3, delay=2):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    print(f"‚ö†Ô∏è Attempt {attempt+1} failed: {e}")
                    time.sleep(delay * (2 ** attempt))  # Exponential backoff
            return None
        return wrapper
    return decorator

class ModelClient:
    # ... previous code ...

    @retry_on_error(max_retries=3, delay=2)
    def generate(self, prompt, max_length=512, temperature=0.7):
        # ... existing generate code ...
```

---

## üìä B∆∞·ªõc 9: Monitor Usage

### Check quota qua API:

```python
import requests

def check_quota(api_token):
    url = "https://huggingface.co/api/usage"
    headers = {"Authorization": f"Bearer {api_token}"}
    response = requests.get(url, headers=headers)
    return response.json()

# Usage
quota = check_quota(os.getenv('HF_API_TOKEN'))
print(f"Monthly quota used: {quota}")
```

### Dashboard:

-   V√†o: https://huggingface.co/settings/billing
-   Xem usage v√† quota remaining

---

## ‚úÖ Checklist

-   [ ] T·∫°o HuggingFace account
-   [ ] Generate Access Token v·ªõi role `read`
-   [ ] Test Inference API v·ªõi Python
-   [ ] Ch·ªçn model ph√π h·ª£p (khuy·∫øn ngh·ªã: `arcee-ai/Arcee-VyLinh`)
-   [ ] Implement `ModelClient` v·ªõi abstraction
-   [ ] Test RAG prompt v·ªõi sample context
-   [ ] Setup error handling v√† retry logic
-   [ ] L∆∞u `HF_API_TOKEN` v√†o `.env`
-   [ ] Verify rate limits v√† quota

---

## üîß Troubleshooting

### L·ªói: "Model is currently loading"

-   **Nguy√™n nh√¢n**: Cold start (model ch∆∞a ƒë∆∞·ª£c load v√†o memory)
-   **Gi·∫£i ph√°p**: ƒê·ª£i 20-30s v√† retry

### L·ªói: "Rate limit exceeded" (429)

-   **Nguy√™n nh√¢n**: V∆∞·ª£t quota ho·∫∑c qu√° nhi·ªÅu requests
-   **Gi·∫£i ph√°p**:
    1. Implement exponential backoff
    2. D√πng cache
    3. Upgrade plan

### L·ªói: "Unauthorized" (401)

-   **Nguy√™n nh√¢n**: Token sai ho·∫∑c expired
-   **Gi·∫£i ph√°p**: Regenerate token v√† update `.env`

### Output quality k√©m:

-   **Gi·∫£i ph√°p**:
    1. Th·ª≠ model l·ªõn h∆°n
    2. Improve prompt engineering
    3. TƒÉng `max_length`
    4. ƒêi·ªÅu ch·ªânh `temperature` (0.5-0.9)

---

## üöÄ Migration Path to AWS (T∆∞∆°ng Lai)

Khi c·∫ßn migrate sang AWS:

1. **Deploy model l√™n AWS SageMaker**
2. **Update `model_client.py`**:

    ```python
    elif self.provider == 'aws':
        return self._call_aws(prompt, max_length, temperature)
    ```

3. **Implement `_call_aws()`**:

    ```python
    def _call_aws(self, prompt, max_length, temperature):
        import boto3
        client = boto3.client('sagemaker-runtime')
        response = client.invoke_endpoint(
            EndpointName=os.getenv('AWS_ENDPOINT_NAME'),
            Body=json.dumps({"inputs": prompt}),
            ContentType='application/json'
        )
        # Parse and return
    ```

4. **Change env var**: `MODEL_PROVIDER=aws`

‚úÖ **Zero code change** trong RAG pipeline, ch·ªâ ƒë·ªïi provider!

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

-   [HuggingFace Inference API Docs](https://huggingface.co/docs/api-inference/index)
-   [Vietnamese Models Hub](https://huggingface.co/models?language=vi&sort=downloads)
-   [Rate Limits & Pricing](https://huggingface.co/pricing)
-   [Best Practices](https://huggingface.co/docs/api-inference/best-practices)

---

**üéâ Xong Phase 1 - Infrastructure Setup!**

B√¢y gi·ªù b·∫°n ƒë√£ c√≥:

-   ‚úÖ Supabase Database
-   ‚úÖ Vector Database (Pinecone/ChromaDB)
-   ‚úÖ HuggingFace LLM API

‚û°Ô∏è Ti·∫øp theo: **Phase 2 - Crawler & Data**
