# ğŸ•·ï¸ VN-Law-Mini Crawler & Data Pipeline

Tools Ä‘á»ƒ import vÃ  xá»­ lÃ½ vÄƒn báº£n phÃ¡p luáº­t Viá»‡t Nam.

---

## ğŸ¯ Chá»©c nÄƒng chÃ­nh

### âœ… Import Documents to Pinecone (Má»šI - Äang sá»­ dá»¥ng)
- Import vÄƒn báº£n tá»« Supabase vÃ o Pinecone vector database
- Táº¡o embeddings vá»›i model multilingual
- Há»— trá»£ RAG (Retrieval-Augmented Generation)
- **Káº¿t quáº£:** 857 text chunks tá»« 87 vÄƒn báº£n
- **Xem:** [QUICK_START.md](./QUICK_START.md)

### âœ… Import to Supabase (Äang hoáº¡t Ä‘á»™ng)
- Import vÄƒn báº£n tá»« files .txt vÃ o Supabase
- Tá»± Ä‘á»™ng parse metadata (loáº¡i vÄƒn báº£n, sá»‘ hiá»‡u, ngÃ y ban hÃ nh)
- Script: `import_manual_documents.py`

### âš ï¸ Web Crawler (Táº¡m dá»«ng)
- Crawler tá»« vbpl.vn gáº·p khÃ³ khÄƒn do PDF embedding
- Xem [CRAWLER-NOTES.md](./CRAWLER-NOTES.md) Ä‘á»ƒ biáº¿t chi tiáº¿t

---

## ğŸ“‹ Files quan trá»ng

**Production Scripts:**
-   **`import_to_pinecone.py`** - Import documents vÃ o Pinecone âœ… MAIN
-   **`import_manual_documents.py`** - Import .txt files vÃ o Supabase âœ…
-   **`QUICK_START.md`** - HÆ°á»›ng dáº«n sá»­ dá»¥ng âœ…

**Legacy/Reference:**
-   **`export_to_supabase.py`** - Export JSON â†’ Supabase
-   **`crawler.py`** - Web crawler cÅ© (tham kháº£o)
-   **`CRAWLER-NOTES.md`** - Ghi chÃº vá» web crawler issues

**Config:**
-   **`.env`** - Credentials (Supabase, Pinecone)

---

## ğŸ”§ Setup

### 1. CÃ i Ä‘áº·t dependencies

## ğŸ”§ Setup

### 1. CÃ i Ä‘áº·t dependencies

```bash
cd backend/rag-service
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh environment

File `.env` trong `backend/rag-service/`:

```bash
# Supabase
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_ANON_KEY=eyJxxx...

# Pinecone
PINECONE_API_KEY=pcsk_xxxxx
PINECONE_INDEX_NAME=vn-law-embeddings

# Embedding Model
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
```

---

## ğŸš€ Quick Start - Import to Pinecone

### BÆ°á»›c 1: Äáº£m báº£o cÃ³ documents trong Supabase

```bash
# Check Supabase dashboard
# Table: documents
# Cáº§n cÃ³: id, ten, noi_dung, mapc
```

### BÆ°á»›c 2: Cháº¡y import script

```bash
python crawler/import_to_pinecone.py
```

### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng RAG service

```bash
cd backend/rag-service
python app.py
```

### BÆ°á»›c 4: Test search

```bash
curl "http://localhost:8001/api/search?query=thanh+niÃªn"
```

**Xem hÆ°á»›ng dáº«n chi tiáº¿t:** [QUICK_START.md](./QUICK_START.md)

---

## ğŸ“Š Káº¿t quáº£

**âœ… ThÃ nh cÃ´ng:**
- 857 text chunks Ä‘Æ°á»£c táº¡o tá»« 87 documents
- Embeddings vá»›i multilingual model (dimension: 768)
- Vector search hoáº¡t Ä‘á»™ng tá»‘t (cosine similarity)
- RAG service sáºµn sÃ ng

---

## ğŸš€ Sá»­ dá»¥ng Export Script (Legacy)

Script `export_to_supabase.py` hoáº¡t Ä‘á»™ng tá»‘t Ä‘á»ƒ import data tá»« JSON vÃ o Supabase.

### Chuáº©n bá»‹ data

Táº¡o 2 files JSON trong `./data/`:

**documents.json**:

```json
[
    {
        "id": "doc-1",
        "mapc": "91-2015-QH13",
        "ten": "Bá»™ luáº­t DÃ¢n sá»± 2015",
        "so_hieu": "91/2015/QH13",
        "loai": "Bá»™ luáº­t",
        "ngay_ban_hanh": "2015-11-24",
        "ngay_hieu_luc": "2017-01-01",
        "trang_thai": "CÃ²n hiá»‡u lá»±c",
        "co_quan_ban_hanh": "Quá»‘c há»™i",
        "nguoi_ky": "Nguyá»…n Sinh HÃ¹ng",
        "noi_dung": "..."
    }
]
```

**articles.json**:

```json
[
    {
        "mapc": "91-2015-QH13-Dieu-1",
        "document_id": "doc-1",
        "ten": "Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh",
        "noi_dung": "Bá»™ luáº­t nÃ y quy Ä‘á»‹nh...",
        "chuong": "",
        "muc": "",
        "thu_tu": 0
    }
]
```

### Cháº¡y export

```bash
python export_to_supabase.py
```

Script sáº½:

-   âœ… Test connection Ä‘áº¿n Supabase
-   âœ… Load data tá»« JSON files
-   âœ… Insert vÃ o tables `documents` vÃ  `articles`
-   âœ… Verify káº¿t quáº£

---

## âš ï¸ Váº¥n Ä‘á» vá»›i Crawler

### Táº¡i sao khÃ´ng crawl Ä‘Æ°á»£c?

1. **Website Ä‘Ã£ thay Ä‘á»•i**: Pháº§n Trung Æ°Æ¡ng (TW) bá»‹ ngáº¯t
2. **Ná»™i dung trong PDF**: Text khÃ´ng extract Ä‘Æ°á»£c tá»« PDF viewer
3. **ItemIDs khÃ´ng theo pattern**: Pháº£i láº¥y tá»« trang danh sÃ¡ch

Chi tiáº¿t Ä‘áº§y Ä‘á»§ â†’ [CRAWLER-NOTES.md](./CRAWLER-NOTES.md)

### Giáº£i phÃ¡p hiá»‡n táº¡i

**Sá»­ dá»¥ng data máº«u** thay vÃ¬ crawler:

-   3 documents: Bá»™ luáº­t DÃ¢n sá»±, HÃ¬nh sá»±, Lao Ä‘á»™ng
-   3 articles vá»›i ná»™i dung Ä‘áº§y Ä‘á»§
-   Äá»§ Ä‘á»ƒ demo vÃ  test há»‡ thá»‘ng

---

## ğŸ“Š Database Schema

### Table: documents

| Column           | Type                | Description   |
| ---------------- | ------------------- | ------------- |
| id               | serial PK           | Auto ID       |
| mapc             | varchar(100) UNIQUE | MÃ£ phÃ¢n cáº¥p   |
| ten              | varchar(500)        | TÃªn vÄƒn báº£n   |
| so_hieu          | varchar(200)        | Sá»‘ hiá»‡u       |
| loai             | varchar(100)        | Loáº¡i vÄƒn báº£n  |
| ngay_ban_hanh    | date                | NgÃ y ban hÃ nh |
| ngay_hieu_luc    | date                | NgÃ y hiá»‡u lá»±c |
| trang_thai       | varchar(100)        | TÃ¬nh tráº¡ng    |
| co_quan_ban_hanh | varchar(200)        | CÆ¡ quan       |
| nguoi_ky         | varchar(200)        | NgÆ°á»i kÃ½      |
| noi_dung         | text                | Ná»™i dung      |
| ghi_chu          | text                | Ghi chÃº       |

### Table: articles

| Column      | Type                | Description   |
| ----------- | ------------------- | ------------- |
| id          | serial PK           | Auto ID       |
| mapc        | varchar(100) UNIQUE | MÃ£ phÃ¢n cáº¥p   |
| document_id | integer FK          | ID vÄƒn báº£n    |
| ten         | varchar(500)        | TÃªn Ä‘iá»u luáº­t |
| noi_dung    | text                | Ná»™i dung      |
| chuong      | varchar(100)        | ChÆ°Æ¡ng        |
| muc         | varchar(100)        | Má»¥c           |
| thu_tu      | integer             | Thá»© tá»±        |

---

## ğŸ”— Tham kháº£o

-   [vbpl.vn](https://vbpl.vn/) - Website VBQPPL
-   [CRAWLER-NOTES.md](./CRAWLER-NOTES.md) - Chi tiáº¿t váº¥n Ä‘á» crawler
-   [Supabase Docs](https://supabase.com/docs) - Database documentation

---

**Tráº¡ng thÃ¡i**: Crawler táº¡m dá»«ng, sá»­ dá»¥ng export script vá»›i data máº«u  
**Cáº­p nháº­t**: 2025-10-05

---

## ğŸ“‹ Chá»©c NÄƒng

-   âœ… Crawl danh sÃ¡ch vÄƒn báº£n tá»« vbpl.vn
-   âœ… Crawl chi tiáº¿t tá»«ng vÄƒn báº£n (metadata + ná»™i dung)
-   âœ… Parse cÃ¡c Ä‘iá»u luáº­t tá»« vÄƒn báº£n
-   âœ… LÆ°u dá»¯ liá»‡u ra JSON
-   âœ… Export sang Supabase PostgreSQL

---

## ğŸ”§ Setup

### 1. CÃ i Ä‘áº·t dependencies

```bash
cd crawler
pip install -r requirements.txt
```

### 2. Cáº¥u hÃ¬nh environment

Táº¡o file `.env` trong thÆ° má»¥c `vn-law-mini/` (root):

```bash
# Supabase credentials
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_SERVICE_KEY=eyJxxx...
```

> âš ï¸ DÃ¹ng `SUPABASE_SERVICE_KEY` (khÃ´ng pháº£i `ANON_KEY`) Ä‘á»ƒ cÃ³ quyá»n write.

---

## ğŸš€ Sá»­ Dá»¥ng

### Option 1: Crawl + Export (2 bÆ°á»›c)

#### BÆ°á»›c 1: Crawl dá»¯ liá»‡u

```bash
python crawler.py
```

Script nÃ y sáº½:

-   Crawl 5 vÄƒn báº£n Ä‘áº§u tiÃªn (cÃ³ thá»ƒ customize)
-   LÆ°u vÃ o `data/documents.json` vÃ  `data/articles.json`

#### BÆ°á»›c 2: Export sang Supabase

```bash
python export_to_supabase.py
```

Script nÃ y sáº½:

-   Äá»c JSON files tá»« `data/`
-   Insert vÃ o Supabase (báº£ng `documents` vÃ  `articles`)
-   Hiá»ƒn thá»‹ progress vÃ  verify

---

### Option 2: Custom Crawl

Táº¡o script riÃªng:

```python
from crawler import LawCrawler

# Init crawler
crawler = LawCrawler(output_dir="./my_data")

# Crawl specific pages
doc_ids = []
for page in range(1, 5):  # Crawl 4 pages
    ids = crawler.crawl_document_list(
        page=page,
        limit=20,
        keyword="dÃ¢n sá»±",  # Optional: filter by keyword
        loai="Luáº­t"        # Optional: filter by document type
    )
    doc_ids.extend(ids)

# Crawl documents
documents, articles = crawler.crawl_batch(doc_ids)

# Save
crawler.save_to_json(documents, 'documents.json')
crawler.save_to_json(articles, 'articles.json')
```

---

## ğŸ“Š Output Format

### documents.json

```json
[
    {
        "id": "123456",
        "ten": "Bá»™ luáº­t DÃ¢n sá»± 2015",
        "so_hieu": "91/2015/QH13",
        "loai": "Bá»™ luáº­t",
        "ngay_ban_hanh": "2015-11-24",
        "ngay_hieu_luc": "2017-01-01",
        "trang_thai": "CÃ²n hiá»‡u lá»±c",
        "co_quan_ban_hanh": "Quá»‘c há»™i",
        "nguoi_ky": "Nguyá»…n Sinh HÃ¹ng",
        "noi_dung": "...",
        "crawled_at": "2024-..."
    }
]
```

### articles.json

```json
[
    {
        "mapc": "91/2015/QH13-Äiá»u-1",
        "document_id": "123456",
        "ten": "Äiá»u 1. Pháº¡m vi Ä‘iá»u chá»‰nh",
        "noi_dung": "Bá»™ luáº­t nÃ y quy Ä‘á»‹nh...",
        "chuong": "",
        "muc": "",
        "thu_tu": 0
    }
]
```

---

## ğŸ—„ï¸ Database Schema

Sau khi export, data sáº½ náº±m trong Supabase:

### Table: documents

| Column           | Type         | Description       |
| ---------------- | ------------ | ----------------- |
| id               | serial PK    | Auto-increment ID |
| ten              | varchar(500) | TÃªn vÄƒn báº£n       |
| so_hieu          | varchar(200) | Sá»‘ hiá»‡u           |
| loai             | varchar(100) | Loáº¡i vÄƒn báº£n      |
| ngay_ban_hanh    | date         | NgÃ y ban hÃ nh     |
| ngay_hieu_luc    | date         | NgÃ y hiá»‡u lá»±c     |
| trang_thai       | varchar(50)  | Tráº¡ng thÃ¡i        |
| co_quan_ban_hanh | varchar(300) | CÆ¡ quan           |
| nguoi_ky         | varchar(200) | NgÆ°á»i kÃ½          |
| noi_dung         | text         | Ná»™i dung toÃ n vÄƒn |
| ghi_chu          | text         | Ghi chÃº           |

### Table: articles

| Column      | Type                | Description       |
| ----------- | ------------------- | ----------------- |
| id          | serial PK           | Auto-increment ID |
| mapc        | varchar(100) UNIQUE | MÃ£ phÃ¡p cháº¿       |
| document_id | int FK              | Link to documents |
| ten         | varchar(500)        | TÃªn Ä‘iá»u          |
| noi_dung    | text                | Ná»™i dung Ä‘iá»u     |
| chuong      | varchar(200)        | ChÆ°Æ¡ng            |
| muc         | varchar(200)        | Má»¥c               |
| thu_tu      | int                 | Thá»© tá»±            |

---

## âš™ï¸ Configuration

### Crawler Settings

Trong `crawler.py`, báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh:

```python
# Number of documents per page
crawler.crawl_document_list(limit=20)

# Delay between requests (giÃ¢y)
time.sleep(1)  # trong hÃ m crawl_batch()

# Regex pattern cho parsing Ä‘iá»u luáº­t
pattern = r'(Äiá»u\s+\d+[a-z]?\.?)\s+(.*?)(?=\nÄiá»u\s+\d+|$)'
```

### Export Settings

Trong `export_to_supabase.py`:

```python
# Batch size (náº¿u crawl nhiá»u)
# CÃ³ thá»ƒ thÃªm batch insert thay vÃ¬ tá»«ng record
```

---

## ğŸ” Troubleshooting

### Error: "SUPABASE_URL not set"

-   Check file `.env` cÃ³ tá»“n táº¡i trong thÆ° má»¥c root
-   Check spelling: `SUPABASE_URL` vÃ  `SUPABASE_SERVICE_KEY`

### Error: "Connection timeout"

-   Website vbpl.vn cÃ³ thá»ƒ cháº­m hoáº·c block
-   TÄƒng timeout: `requests.get(url, timeout=30)`
-   ThÃªm delay giá»¯a cÃ¡c requests

### Error: "Permission denied" khi insert Supabase

-   DÃ¹ng `SUPABASE_SERVICE_KEY` thay vÃ¬ `SUPABASE_ANON_KEY`
-   Check RLS (Row Level Security) Ä‘Ã£ táº¯t cho dev

### KhÃ´ng parse Ä‘Æ°á»£c Ä‘iá»u luáº­t

-   Kiá»ƒm tra format vÄƒn báº£n cÃ³ khÃ¡c khÃ´ng
-   Äiá»u chá»‰nh regex pattern trong `parse_articles()`
-   Some documents khÃ´ng cÃ³ cáº¥u trÃºc "Äiá»u X"

### Duplicate key error

-   Báº£ng `articles` cÃ³ constraint UNIQUE trÃªn `mapc`
-   Script tá»± Ä‘á»™ng skip duplicates
-   Náº¿u cáº§n re-import, xÃ³a data cÅ© trÆ°á»›c:
    ```sql
    DELETE FROM articles;
    DELETE FROM documents;
    ```

---

## ğŸ“ˆ Performance Tips

### Crawl nhiá»u vÄƒn báº£n

```python
# Crawl 100 vÄƒn báº£n
doc_ids = []
for page in range(1, 6):  # 5 pages * 20 = 100
    ids = crawler.crawl_document_list(page=page, limit=20)
    doc_ids.extend(ids)

# Crawl theo batch nhá» Ä‘á»ƒ trÃ¡nh timeout
batch_size = 10
for i in range(0, len(doc_ids), batch_size):
    batch = doc_ids[i:i+batch_size]
    documents, articles = crawler.crawl_batch(batch)

    # Save incrementally
    crawler.save_to_json(documents, f'documents_batch_{i}.json')
    crawler.save_to_json(articles, f'articles_batch_{i}.json')
```

### Export nhanh hÆ¡n

DÃ¹ng batch insert trong Supabase:

```python
# Thay vÃ¬ insert tá»«ng record
supabase.table('documents').insert(data).execute()

# DÃ¹ng bulk insert (náº¿u Supabase support)
supabase.table('documents').insert(list_of_documents).execute()
```

---

## ğŸ§ª Testing

Test script cÃ³ sáºµn:

```bash
# Test crawl 1 vÄƒn báº£n
python test_crawler.py
```

Hoáº·c test manual:

```python
from crawler import LawCrawler

crawler = LawCrawler()

# Test crawl 1 document
doc = crawler.crawl_document_detail("123456")
print(doc)

# Test parse articles
articles = crawler.parse_articles(doc)
print(f"Found {len(articles)} articles")
```

---

## ğŸ“ Notes

### Legal & Ethical

-   âœ… Data tá»« vbpl.vn lÃ  cÃ´ng khai
-   âœ… Crawler tuÃ¢n thá»§ robots.txt
-   âœ… CÃ³ delay giá»¯a requests (avoid DDoS)
-   âš ï¸ Chá»‰ dÃ¹ng cho má»¥c Ä‘Ã­ch nghiÃªn cá»©u/giÃ¡o dá»¥c

### Limitations

-   Chá»‰ crawl Ä‘Æ°á»£c vÄƒn báº£n tá»« vbpl.vn
-   Má»™t sá»‘ vÄƒn báº£n cÃ³ format khÃ¡c nhau â†’ parse khÃ´ng Ä‘Æ°á»£c
-   KhÃ´ng crawl Ä‘Æ°á»£c attachments (PDF, DOC)
-   KhÃ´ng track changes/updates cá»§a vÄƒn báº£n

---

## ğŸ”— Related

-   [Supabase Setup Guide](../docs/01-SETUP-SUPABASE.md)
-   [Database Schema](../infrastructure/supabase-schema.sql)
-   Main Project: [VN-Law-Advisor](https://github.com/CTU-LinguTechies/VN-Law-Advisor)

---

## ğŸ“§ Support

Issues? Report táº¡i GitHub Issues hoáº·c check documentation.
