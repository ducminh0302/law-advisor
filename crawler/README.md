# üï∑Ô∏è VN-Law-Mini Crawler & Data Pipeline

Tools ƒë·ªÉ import v√† x·ª≠ l√Ω vƒÉn b·∫£n ph√°p lu·∫≠t Vi·ªát Nam.

---

## üéØ Ch·ª©c nƒÉng ch√≠nh

### ‚úÖ Import Documents to Pinecone (M·ªöI - ƒêang s·ª≠ d·ª•ng)
- Import vƒÉn b·∫£n t·ª´ Supabase v√†o Pinecone vector database
- T·∫°o embeddings v·ªõi model multilingual
- H·ªó tr·ª£ RAG (Retrieval-Augmented Generation)
- **K·∫øt qu·∫£:** 857 text chunks t·ª´ 87 vƒÉn b·∫£n
- **Xem:** [QUICK_START.md](./QUICK_START.md)

### ‚úÖ Import to Supabase (ƒêang ho·∫°t ƒë·ªông)
- Import vƒÉn b·∫£n t·ª´ files .txt v√†o Supabase
- T·ª± ƒë·ªông parse metadata (lo·∫°i vƒÉn b·∫£n, s·ªë hi·ªáu, ng√†y ban h√†nh)
- Script: `import_manual_documents.py`

### ‚ö†Ô∏è Web Crawler (T·∫°m d·ª´ng)
- Crawler t·ª´ vbpl.vn g·∫∑p kh√≥ khƒÉn do PDF embedding
- Xem [CRAWLER-NOTES.md](./CRAWLER-NOTES.md) ƒë·ªÉ bi·∫øt chi ti·∫øt

---

## üìã Files quan tr·ªçng

**Production Scripts:**
-   **`import_to_pinecone.py`** - Import documents v√†o Pinecone ‚úÖ MAIN
-   **`import_manual_documents.py`** - Import .txt files v√†o Supabase ‚úÖ
-   **`QUICK_START.md`** - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng ‚úÖ

**Legacy/Reference:**
-   **`export_to_supabase.py`** - Export JSON ‚Üí Supabase
-   **`crawler.py`** - Web crawler c≈© (tham kh·∫£o)
-   **`CRAWLER-NOTES.md`** - Ghi ch√∫ v·ªÅ web crawler issues

**Config:**
-   **`.env`** - Credentials (Supabase, Pinecone)

---

## üîß Setup

### 1. C√†i ƒë·∫∑t dependencies

## üîß Setup

### 1. C√†i ƒë·∫∑t dependencies

```bash
cd backend/rag-service
pip install -r requirements.txt
```

### 2. C·∫•u h√¨nh environment

File `.env` trong `backend/rag-service/`:

```bash
# Supabase
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_ANON_KEY=eyJxxx...

# Pinecone
PINECONE_API_KEY=pcsk_xxxxx
PINECONE_INDEX_NAME=vn-law-embeddings

# Embedding Model
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-mpnet-base-v2
```

---

## üöÄ Quick Start - Import to Pinecone

### B∆∞·ªõc 1: ƒê·∫£m b·∫£o c√≥ documents trong Supabase

```bash
# Check Supabase dashboard
# Table: documents
# C·∫ßn c√≥: id, ten, noi_dung, mapc
```

### B∆∞·ªõc 2: Ch·∫°y import script

```bash
python crawler/import_to_pinecone.py
```

### B∆∞·ªõc 3: Kh·ªüi ƒë·ªông RAG service

```bash
cd backend/rag-service
python app.py
```

### B∆∞·ªõc 4: Test search

```bash
curl "http://localhost:8001/api/search?query=thanh+ni√™n"
```

**Xem h∆∞·ªõng d·∫´n chi ti·∫øt:** [QUICK_START.md](./QUICK_START.md)

---

## üìä K·∫øt qu·∫£

**‚úÖ Th√†nh c√¥ng:**
- 857 text chunks ƒë∆∞·ª£c t·∫°o t·ª´ 87 documents
- Embeddings v·ªõi multilingual model (dimension: 768)
- Vector search ho·∫°t ƒë·ªông t·ªët (cosine similarity)
- RAG service s·∫µn s√†ng

---

## üöÄ S·ª≠ d·ª•ng Export Script (Legacy)

Script `export_to_supabase.py` ho·∫°t ƒë·ªông t·ªët ƒë·ªÉ import data t·ª´ JSON v√†o Supabase.

### Chu·∫©n b·ªã data

T·∫°o 2 files JSON trong `./data/`:

**documents.json**:

```json
[
    {
        "id": "doc-1",
        "mapc": "91-2015-QH13",
        "ten": "B·ªô lu·∫≠t D√¢n s·ª± 2015",
        "so_hieu": "91/2015/QH13",
        "loai": "B·ªô lu·∫≠t",
        "ngay_ban_hanh": "2015-11-24",
        "ngay_hieu_luc": "2017-01-01",
        "trang_thai": "C√≤n hi·ªáu l·ª±c",
        "co_quan_ban_hanh": "Qu·ªëc h·ªôi",
        "nguoi_ky": "Nguy·ªÖn Sinh H√πng",
        "noi_dung": "..."
    }
]
```

**articles.json**:

```json
[
    {
        "mapc": "91-2015-QH13-Dieu-1",
        "document_id": "doc-1",
        "ten": "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh",
        "noi_dung": "B·ªô lu·∫≠t n√†y quy ƒë·ªãnh...",
        "chuong": "",
        "muc": "",
        "thu_tu": 0
    }
]
```

### Ch·∫°y export

```bash
python export_to_supabase.py
```

Script s·∫Ω:

-   ‚úÖ Test connection ƒë·∫øn Supabase
-   ‚úÖ Load data t·ª´ JSON files
-   ‚úÖ Insert v√†o tables `documents` v√† `articles`
-   ‚úÖ Verify k·∫øt qu·∫£

---

## ‚ö†Ô∏è V·∫•n ƒë·ªÅ v·ªõi Crawler

### T·∫°i sao kh√¥ng crawl ƒë∆∞·ª£c?

1. **Website ƒë√£ thay ƒë·ªïi**: Ph·∫ßn Trung ∆∞∆°ng (TW) b·ªã ng·∫Øt
2. **N·ªôi dung trong PDF**: Text kh√¥ng extract ƒë∆∞·ª£c t·ª´ PDF viewer
3. **ItemIDs kh√¥ng theo pattern**: Ph·∫£i l·∫•y t·ª´ trang danh s√°ch

Chi ti·∫øt ƒë·∫ßy ƒë·ªß ‚Üí [CRAWLER-NOTES.md](./CRAWLER-NOTES.md)

### Gi·∫£i ph√°p hi·ªán t·∫°i

**S·ª≠ d·ª•ng data m·∫´u** thay v√¨ crawler:

-   3 documents: B·ªô lu·∫≠t D√¢n s·ª±, H√¨nh s·ª±, Lao ƒë·ªông
-   3 articles v·ªõi n·ªôi dung ƒë·∫ßy ƒë·ªß
-   ƒê·ªß ƒë·ªÉ demo v√† test h·ªá th·ªëng

---

## üìä Database Schema

### Table: documents

| Column           | Type                | Description   |
| ---------------- | ------------------- | ------------- |
| id               | serial PK           | Auto ID       |
| mapc             | varchar(100) UNIQUE | M√£ ph√¢n c·∫•p   |
| ten              | varchar(500)        | T√™n vƒÉn b·∫£n   |
| so_hieu          | varchar(200)        | S·ªë hi·ªáu       |
| loai             | varchar(100)        | Lo·∫°i vƒÉn b·∫£n  |
| ngay_ban_hanh    | date                | Ng√†y ban h√†nh |
| ngay_hieu_luc    | date                | Ng√†y hi·ªáu l·ª±c |
| trang_thai       | varchar(100)        | T√¨nh tr·∫°ng    |
| co_quan_ban_hanh | varchar(200)        | C∆° quan       |
| nguoi_ky         | varchar(200)        | Ng∆∞·ªùi k√Ω      |
| noi_dung         | text                | N·ªôi dung      |
| ghi_chu          | text                | Ghi ch√∫       |

### Table: articles

| Column      | Type                | Description   |
| ----------- | ------------------- | ------------- |
| id          | serial PK           | Auto ID       |
| mapc        | varchar(100) UNIQUE | M√£ ph√¢n c·∫•p   |
| document_id | integer FK          | ID vƒÉn b·∫£n    |
| ten         | varchar(500)        | T√™n ƒëi·ªÅu lu·∫≠t |
| noi_dung    | text                | N·ªôi dung      |
| chuong      | varchar(100)        | Ch∆∞∆°ng        |
| muc         | varchar(100)        | M·ª•c           |
| thu_tu      | integer             | Th·ª© t·ª±        |

---

## üîó Tham kh·∫£o

-   [vbpl.vn](https://vbpl.vn/) - Website VBQPPL
-   [CRAWLER-NOTES.md](./CRAWLER-NOTES.md) - Chi ti·∫øt v·∫•n ƒë·ªÅ crawler
-   [Supabase Docs](https://supabase.com/docs) - Database documentation

---

**Tr·∫°ng th√°i**: Crawler t·∫°m d·ª´ng, s·ª≠ d·ª•ng export script v·ªõi data m·∫´u  
**C·∫≠p nh·∫≠t**: 2025-10-05

---

## üìã Ch·ª©c NƒÉng

-   ‚úÖ Crawl danh s√°ch vƒÉn b·∫£n t·ª´ vbpl.vn
-   ‚úÖ Crawl chi ti·∫øt t·ª´ng vƒÉn b·∫£n (metadata + n·ªôi dung)
-   ‚úÖ Parse c√°c ƒëi·ªÅu lu·∫≠t t·ª´ vƒÉn b·∫£n
-   ‚úÖ L∆∞u d·ªØ li·ªáu ra JSON
-   ‚úÖ Export sang Supabase PostgreSQL

---

## üîß Setup

### 1. C√†i ƒë·∫∑t dependencies

```bash
cd crawler
pip install -r requirements.txt
```

### 2. C·∫•u h√¨nh environment

T·∫°o file `.env` trong th∆∞ m·ª•c `vn-law-mini/` (root):

```bash
# Supabase credentials
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_SERVICE_KEY=eyJxxx...
```

> ‚ö†Ô∏è D√πng `SUPABASE_SERVICE_KEY` (kh√¥ng ph·∫£i `ANON_KEY`) ƒë·ªÉ c√≥ quy·ªÅn write.

---

## üöÄ S·ª≠ D·ª•ng

### Option 1: Crawl + Export (2 b∆∞·ªõc)

#### B∆∞·ªõc 1: Crawl d·ªØ li·ªáu

```bash
python crawler.py
```

Script n√†y s·∫Ω:

-   Crawl 5 vƒÉn b·∫£n ƒë·∫ßu ti√™n (c√≥ th·ªÉ customize)
-   L∆∞u v√†o `data/documents.json` v√† `data/articles.json`

#### B∆∞·ªõc 2: Export sang Supabase

```bash
python export_to_supabase.py
```

Script n√†y s·∫Ω:

-   ƒê·ªçc JSON files t·ª´ `data/`
-   Insert v√†o Supabase (b·∫£ng `documents` v√† `articles`)
-   Hi·ªÉn th·ªã progress v√† verify

---

### Option 2: Custom Crawl

T·∫°o script ri√™ng:

```python
from crawler import LawCrawler

# Init crawler
crawler = LawCrawler(output_dir="./my_data")

# Crawl specific pages
doc_ids = []
for page in range(1, 5):  # Crawl 4 pages
    ids = crawler.crawl_document_list(
        page=page,
        limit=20,
        keyword="d√¢n s·ª±",  # Optional: filter by keyword
        loai="Lu·∫≠t"        # Optional: filter by document type
    )
    doc_ids.extend(ids)

# Crawl documents
documents, articles = crawler.crawl_batch(doc_ids)

# Save
crawler.save_to_json(documents, 'documents.json')
crawler.save_to_json(articles, 'articles.json')
```

---

## üìä Output Format

### documents.json

```json
[
    {
        "id": "123456",
        "ten": "B·ªô lu·∫≠t D√¢n s·ª± 2015",
        "so_hieu": "91/2015/QH13",
        "loai": "B·ªô lu·∫≠t",
        "ngay_ban_hanh": "2015-11-24",
        "ngay_hieu_luc": "2017-01-01",
        "trang_thai": "C√≤n hi·ªáu l·ª±c",
        "co_quan_ban_hanh": "Qu·ªëc h·ªôi",
        "nguoi_ky": "Nguy·ªÖn Sinh H√πng",
        "noi_dung": "...",
        "crawled_at": "2024-..."
    }
]
```

### articles.json

```json
[
    {
        "mapc": "91/2015/QH13-ƒêi·ªÅu-1",
        "document_id": "123456",
        "ten": "ƒêi·ªÅu 1. Ph·∫°m vi ƒëi·ªÅu ch·ªânh",
        "noi_dung": "B·ªô lu·∫≠t n√†y quy ƒë·ªãnh...",
        "chuong": "",
        "muc": "",
        "thu_tu": 0
    }
]
```

---

## üóÑÔ∏è Database Schema

Sau khi export, data s·∫Ω n·∫±m trong Supabase:

### Table: documents

| Column           | Type         | Description       |
| ---------------- | ------------ | ----------------- |
| id               | serial PK    | Auto-increment ID |
| ten              | varchar(500) | T√™n vƒÉn b·∫£n       |
| so_hieu          | varchar(200) | S·ªë hi·ªáu           |
| loai             | varchar(100) | Lo·∫°i vƒÉn b·∫£n      |
| ngay_ban_hanh    | date         | Ng√†y ban h√†nh     |
| ngay_hieu_luc    | date         | Ng√†y hi·ªáu l·ª±c     |
| trang_thai       | varchar(50)  | Tr·∫°ng th√°i        |
| co_quan_ban_hanh | varchar(300) | C∆° quan           |
| nguoi_ky         | varchar(200) | Ng∆∞·ªùi k√Ω          |
| noi_dung         | text         | N·ªôi dung to√†n vƒÉn |
| ghi_chu          | text         | Ghi ch√∫           |

### Table: articles

| Column      | Type                | Description       |
| ----------- | ------------------- | ----------------- |
| id          | serial PK           | Auto-increment ID |
| mapc        | varchar(100) UNIQUE | M√£ ph√°p ch·∫ø       |
| document_id | int FK              | Link to documents |
| ten         | varchar(500)        | T√™n ƒëi·ªÅu          |
| noi_dung    | text                | N·ªôi dung ƒëi·ªÅu     |
| chuong      | varchar(200)        | Ch∆∞∆°ng            |
| muc         | varchar(200)        | M·ª•c               |
| thu_tu      | int                 | Th·ª© t·ª±            |

---

## ‚öôÔ∏è Configuration

### Crawler Settings

Trong `crawler.py`, b·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh:

```python
# Number of documents per page
crawler.crawl_document_list(limit=20)

# Delay between requests (gi√¢y)
time.sleep(1)  # trong h√†m crawl_batch()

# Regex pattern cho parsing ƒëi·ªÅu lu·∫≠t
pattern = r'(ƒêi·ªÅu\s+\d+[a-z]?\.?)\s+(.*?)(?=\nƒêi·ªÅu\s+\d+|$)'
```

### Export Settings

Trong `export_to_supabase.py`:

```python
# Batch size (n·∫øu crawl nhi·ªÅu)
# C√≥ th·ªÉ th√™m batch insert thay v√¨ t·ª´ng record
```

---

## üîç Troubleshooting

### Error: "SUPABASE_URL not set"

-   Check file `.env` c√≥ t·ªìn t·∫°i trong th∆∞ m·ª•c root
-   Check spelling: `SUPABASE_URL` v√† `SUPABASE_SERVICE_KEY`

### Error: "Connection timeout"

-   Website vbpl.vn c√≥ th·ªÉ ch·∫≠m ho·∫∑c block
-   TƒÉng timeout: `requests.get(url, timeout=30)`
-   Th√™m delay gi·ªØa c√°c requests

### Error: "Permission denied" khi insert Supabase

-   D√πng `SUPABASE_SERVICE_KEY` thay v√¨ `SUPABASE_ANON_KEY`
-   Check RLS (Row Level Security) ƒë√£ t·∫Øt cho dev

### Kh√¥ng parse ƒë∆∞·ª£c ƒëi·ªÅu lu·∫≠t

-   Ki·ªÉm tra format vƒÉn b·∫£n c√≥ kh√°c kh√¥ng
-   ƒêi·ªÅu ch·ªânh regex pattern trong `parse_articles()`
-   Some documents kh√¥ng c√≥ c·∫•u tr√∫c "ƒêi·ªÅu X"

### Duplicate key error

-   B·∫£ng `articles` c√≥ constraint UNIQUE tr√™n `mapc`
-   Script t·ª± ƒë·ªông skip duplicates
-   N·∫øu c·∫ßn re-import, x√≥a data c≈© tr∆∞·ªõc:
    ```sql
    DELETE FROM articles;
    DELETE FROM documents;
    ```

---

## üìà Performance Tips

### Crawl nhi·ªÅu vƒÉn b·∫£n

```python
# Crawl 100 vƒÉn b·∫£n
doc_ids = []
for page in range(1, 6):  # 5 pages * 20 = 100
    ids = crawler.crawl_document_list(page=page, limit=20)
    doc_ids.extend(ids)

# Crawl theo batch nh·ªè ƒë·ªÉ tr√°nh timeout
batch_size = 10
for i in range(0, len(doc_ids), batch_size):
    batch = doc_ids[i:i+batch_size]
    documents, articles = crawler.crawl_batch(batch)

    # Save incrementally
    crawler.save_to_json(documents, f'documents_batch_{i}.json')
    crawler.save_to_json(articles, f'articles_batch_{i}.json')
```

### Export nhanh h∆°n

D√πng batch insert trong Supabase:

```python
# Thay v√¨ insert t·ª´ng record
supabase.table('documents').insert(data).execute()

# D√πng bulk insert (n·∫øu Supabase support)
supabase.table('documents').insert(list_of_documents).execute()
```

---

## üß™ Testing

Test script c√≥ s·∫µn:

```bash
# Test crawl 1 vƒÉn b·∫£n
python test_crawler.py
```

Ho·∫∑c test manual:

```python
from crawler import LawCrawler

crawler = LawCrawler()

# Test crawl 1 document
doc = crawler.crawl_document_detail("123456")
print(doc)

# Test parse articles
articles = crawler.parse_articles(doc)
print(f"Found {len(articles)} articles")
```

---

## üìù Notes

### Legal & Ethical

-   ‚úÖ Data t·ª´ vbpl.vn l√† c√¥ng khai
-   ‚úÖ Crawler tu√¢n th·ªß robots.txt
-   ‚úÖ C√≥ delay gi·ªØa requests (avoid DDoS)
-   ‚ö†Ô∏è Ch·ªâ d√πng cho m·ª•c ƒë√≠ch nghi√™n c·ª©u/gi√°o d·ª•c

### Limitations

-   Ch·ªâ crawl ƒë∆∞·ª£c vƒÉn b·∫£n t·ª´ vbpl.vn
-   M·ªôt s·ªë vƒÉn b·∫£n c√≥ format kh√°c nhau ‚Üí parse kh√¥ng ƒë∆∞·ª£c
-   Kh√¥ng crawl ƒë∆∞·ª£c attachments (PDF, DOC)
-   Kh√¥ng track changes/updates c·ªßa vƒÉn b·∫£n

---

## üîó Related

-   [Supabase Setup Guide](../docs/01-SETUP-SUPABASE.md)
-   [Database Schema](../infrastructure/supabase-schema.sql)
-   Main Project: [VN-Law-Advisor](https://github.com/CTU-LinguTechies/VN-Law-Advisor)

---

## üìß Support

Issues? Report t·∫°i GitHub Issues ho·∫∑c check documentation.
