"""
Import Documents to Pinecone

Script n√†y ƒë·ªçc vƒÉn b·∫£n t·ª´ Supabase v√† import v√†o Pinecone
ƒë·ªÉ RAG service c√≥ th·ªÉ search ƒë∆∞·ª£c.
"""

import os
import sys
from dotenv import load_dotenv
from supabase import create_client, Client

# Add rag-service src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend', 'rag-service', 'src'))

# Patch torch before importing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend', 'rag-service'))
import patch_torch

from models.vector_store import VectorStore

# Load .env t·ª´ rag-service
env_path = os.path.join(os.path.dirname(__file__), '..', 'backend', 'rag-service', '.env')
load_dotenv(env_path)
print(f"Loaded .env from: {env_path}")


def fetch_documents_from_supabase():
    """Fetch documents t·ª´ Supabase"""
    print("üì• Fetching documents from Supabase...")
    
    supabase_url = os.getenv('SUPABASE_URL')
    supabase_key = os.getenv('SUPABASE_ANON_KEY')
    
    if not supabase_url or not supabase_key:
        print("‚ùå Missing Supabase credentials!")
        return []
    
    try:
        supabase: Client = create_client(supabase_url, supabase_key)
        
        # Fetch all documents
        response = supabase.table('documents').select('*').execute()
        docs = response.data
        
        # Convert to format compatible with vectorize
        documents = []
        for doc in docs:
            # Make sure we have a valid ID
            doc_id = doc.get('id')
            if not doc_id:
                continue
                
            mapc = doc.get('mapc')
            if not mapc or mapc == '':
                mapc = f"doc-{doc_id}"
            
            document = {
                'mapc': mapc,
                'ten': doc.get('ten', 'Untitled'),
                'noidung': doc.get('noi_dung', '') or doc.get('content', ''),
                'document_id': doc_id,
                'document_type': doc.get('loai', ''),
                'document_number': doc.get('so_hieu', ''),
            }
            
            # Only add if has content
            if document['noidung'] and len(document['noidung']) > 50:
                documents.append(document)
        
        print(f"‚úÖ Fetched {len(documents)} documents")
        return documents
        
    except Exception as e:
        print(f"‚ùå Error fetching documents: {e}")
        import traceback
        traceback.print_exc()
        return []


def split_into_chunks(documents, chunk_size=1000):
    """Chia vƒÉn b·∫£n d√†i th√†nh c√°c chunks nh·ªè h∆°n"""
    chunks = []
    
    for doc in documents:
        content = doc['noidung']
        
        # N·∫øu vƒÉn b·∫£n ng·∫Øn, gi·ªØ nguy√™n
        if len(content) <= chunk_size:
            chunks.append(doc)
            continue
        
        # Chia th√†nh c√°c chunks
        words = content.split()
        current_chunk = []
        current_length = 0
        chunk_idx = 1
        
        for word in words:
            current_chunk.append(word)
            current_length += len(word) + 1
            
            if current_length >= chunk_size:
                # T·∫°o chunk m·ªõi
                chunk = doc.copy()
                chunk['mapc'] = f"{doc['mapc']}-chunk{chunk_idx}"
                chunk['ten'] = f"{doc['ten']} (Ph·∫ßn {chunk_idx})"
                chunk['noidung'] = ' '.join(current_chunk)
                chunks.append(chunk)
                
                # Reset
                current_chunk = []
                current_length = 0
                chunk_idx += 1
        
        # Th√™m ph·∫ßn c√≤n l·∫°i
        if current_chunk:
            chunk = doc.copy()
            chunk['mapc'] = f"{doc['mapc']}-chunk{chunk_idx}"
            chunk['ten'] = f"{doc['ten']} (Ph·∫ßn {chunk_idx})"
            chunk['noidung'] = ' '.join(current_chunk)
            chunks.append(chunk)
    
    return chunks


def main():
    print("="*60)
    print("üìö IMPORT DOCUMENTS TO PINECONE")
    print("="*60)
    
    # Check environment
    provider = os.getenv('VECTOR_DB_PROVIDER', 'pinecone')
    print(f"\nüîß Vector DB Provider: {provider}")
    
    if provider == 'pinecone':
        api_key = os.getenv('PINECONE_API_KEY')
        index_name = os.getenv('PINECONE_INDEX_NAME')
        
        if not api_key or not index_name:
            print("\n‚ùå Missing Pinecone configuration!")
            print("   Please set in .env:")
            print("   - PINECONE_API_KEY")
            print("   - PINECONE_INDEX_NAME")
            sys.exit(1)
        
        print(f"   Index: {index_name}")
    
    # Fetch documents
    documents = fetch_documents_from_supabase()
    
    if not documents:
        print("\n‚ùå No documents found!")
        sys.exit(1)
    
    # Split into chunks
    print(f"\n‚úÇÔ∏è  Splitting documents into chunks...")
    chunks = split_into_chunks(documents, chunk_size=1500)
    print(f"   Created {len(chunks)} chunks from {len(documents)} documents")
    
    # Initialize vector store
    print(f"\nüîå Initializing {provider} vector store...")
    try:
        vector_store = VectorStore(provider=provider)
        print("‚úÖ Vector store initialized")
    except Exception as e:
        print(f"‚ùå Error: {e}")
        sys.exit(1)
    
    # Confirm
    print(f"\n‚ö†Ô∏è  About to create embeddings for {len(chunks)} text chunks")
    print(f"   This may take 5-10 minutes...")
    print(f"   Embedding model: paraphrase-multilingual-mpnet-base-v2")
    
    confirm = input("\n   Continue? (y/n): ")
    if confirm.lower() != 'y':
        print("‚ùå Cancelled")
        sys.exit(0)
    
    # Upload to vector store
    print(f"\nüöÄ Creating embeddings and uploading to {provider}...")
    print("   This will take a while, please wait...")
    
    try:
        # Batch processing
        batch_size = 50
        total_uploaded = 0
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            count = vector_store.upsert_batch(batch)
            total_uploaded += count
            print(f"   Progress: {total_uploaded}/{len(chunks)} chunks uploaded")
        
        print(f"\n‚úÖ SUCCESS! Uploaded {total_uploaded} embeddings to {provider}")
        
    except Exception as e:
        print(f"\n‚ùå Error during upload: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Test search
    print("\n" + "="*60)
    print("üîç TESTING VECTOR SEARCH")
    print("="*60)
    
    test_queries = [
        "thanh ni√™n kh·ªüi nghi·ªáp",
        "ƒëo√†n vi√™n",
        "k·∫ø ho·∫°ch ho·∫°t ƒë·ªông"
    ]
    
    for query in test_queries:
        print(f"\nüìù Query: '{query}'")
        try:
            results = vector_store.search(query, top_k=3)
            print(f"   Found {len(results)} results:")
            for i, result in enumerate(results, 1):
                score = result.get('score', 0)
                title = result.get('ten', 'Unknown')
                print(f"   {i}. {title[:60]}... (score: {score:.3f})")
        except Exception as e:
            print(f"   ‚ùå Search error: {e}")
    
    print("\n" + "="*60)
    print("‚úÖ DONE! Pinecone is ready for RAG service")
    print("="*60)
    print("\nüí° Next steps:")
    print("   1. Start RAG service: python backend/rag-service/app.py")
    print("   2. Test search API at: http://localhost:8001/api/search")
    print("\n")


if __name__ == '__main__':
    main()
