# GI·∫¢I PH√ÅP: D√πng Ollama - Local LLM

## HuggingFace Inference API kh√¥ng ho·∫°t ƒë·ªông!
T·∫•t c·∫£ models tr·∫£ v·ªÅ 404. Free tier c√≥ th·ªÉ ƒë√£ b·ªã gi·ªõi h·∫°n.

## ‚úÖ Gi·∫£i ph√°p: OLLAMA (Ch·∫°y LLM local)

### B∆∞·ªõc 1: C√†i Ollama
1. Download: https://ollama.com/download/windows
2. C√†i ƒë·∫∑t Ollama
3. M·ªü PowerShell v√† ch·∫°y:
```powershell
ollama serve
```

### B∆∞·ªõc 2: Pull model
```powershell
# Model nh·ªè (1.3GB)
ollama pull gemma:2b

# Ho·∫∑c model l·ªõn h∆°n (4.1GB)
ollama pull llama3.2

# Ho·∫∑c model ti·∫øng Vi·ªát
ollama pull vinai/phobert
```

### B∆∞·ªõc 3: Test Ollama
```powershell
ollama run gemma:2b "Xin ch√†o"
```

### B∆∞·ªõc 4: C·∫≠p nh·∫≠t code RAG Service

ƒê·ªïi `backend/rag-service/app.py`:

```python
def call_ollama_api(prompt, max_tokens=512):
    """G·ªçi Ollama local API"""
    try:
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                'model': 'gemma:2b',
                'prompt': prompt,
                'stream': False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            return response.json().get('response', '')
        return None
    except Exception as e:
        print(f"Ollama error: {e}")
        return None
```

### B∆∞·ªõc 5: Update `.env`
```bash
# Thay v√¨ HuggingFace, d√πng Ollama
MODEL_PROVIDER=ollama
OLLAMA_API_URL=http://localhost:11434
OLLAMA_MODEL=gemma:2b
```

## ∆Øu ƒëi·ªÉm Ollama:
‚úÖ Ch·∫°y local - kh√¥ng c·∫ßn API key
‚úÖ Mi·ªÖn ph√≠ 100%
‚úÖ Nhanh, kh√¥ng gi·ªõi h·∫°n requests
‚úÖ H·ªó tr·ª£ nhi·ªÅu models: Llama, Gemma, Mistral, Phi...
‚úÖ D·ªÖ switch model

## B·∫°n mu·ªën t√¥i:
1. ‚úÖ C·∫≠p nh·∫≠t code ƒë·ªÉ d√πng Ollama
2. ‚è≠Ô∏è T√¨m API kh√°c (OpenRouter, Groq - mi·ªÖn ph√≠)
3. üîß Debug ti·∫øp HuggingFace

Ch·ªçn ph∆∞∆°ng √°n n√†o?
